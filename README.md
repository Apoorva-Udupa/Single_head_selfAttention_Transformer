# Single_head_selfAttention_Transformer
Created Single head self attention transformer using Python, Numpy

This repository contains the implementation of a Single Head Self Attention Transformer Neural Network, a simplified version of the Transformer model introduced in the "Attention Is All You Need" paper. This project focuses on demonstrating the core concepts of self-attention mechanisms in neural networks.

## Project Overview

The goal of this project is to provide a clear and concise implementation of a single head self-attention mechanism within a Transformer architecture. This serves as an educational tool for those interested in understanding and applying Transformers in their machine learning projects.

## Features

- Implementation of a Single Head Self Attention module.
- Example usage of the Transformer for a sample task (e.g., sequence classification).
- Performance analysis and comparison with other neural network architectures.

## Getting Started

These instructions will guide you on how to set up and run the project on your local machine.

### Prerequisites

Ensure you have Python 3.x installed along with the following packages:
- numpy
- torch

### Installation

Clone the repository to your local machine:

```bash
git clone https://github.com/Apoorva-Udupa/Single_Head_Self_Attention_Transformer_NN.git
cd Single_Head_Self_Attention_Transformer_NN
```
